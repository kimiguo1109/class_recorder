# Tasks: è¯¾å ‚å½•éŸ³è½¬å½•ä¸ AI å­¦ä¹ åŠ©æ‰‹

## Phase 1: æ ¸å¿ƒè½¬å½• MVP

### Task 1.1: åˆå§‹åŒ–é¡¹ç›®ç»“æ„
- [ ] **åˆ›å»ºåç«¯é¡¹ç›®ç»“æ„**
  - æ–‡ä»¶: `backend/main.py`, `backend/requirements.txt`, `backend/.env.example`
  - å®‰è£…ä¾èµ–: FastAPI, uvicorn, websockets, google-generativeai, python-dotenv
  - é…ç½® FastAPI åº”ç”¨å’Œ CORS

- [ ] **åˆ›å»ºå‰ç«¯é¡¹ç›®ç»“æ„**
  - æ–‡ä»¶: `frontend/` (Vite + React + TypeScript)
  - å®‰è£…ä¾èµ–: React, TypeScript, TailwindCSS
  - é…ç½® TailwindCSS å’ŒåŸºç¡€æ ·å¼

**_Prompt:**
```
Role: Full-stack developer setting up a new Python + React project

Task: Initialize the class-recorder project with proper structure
- Backend: Create FastAPI app with WebSocket support and Gemini API integration
- Frontend: Create Vite React TypeScript project with TailwindCSS
- Setup environment variables and configuration files

Restrictions:
- Use Python 3.11+
- Use React 18+ with TypeScript
- Do not install unnecessary dependencies
- Follow the design specifications in design.md

_Requirements: FR-1.1, FR-1.3
_Leverage: None (new project)

Success: 
- Backend starts on localhost:8000
- Frontend starts on localhost:5173
- No compilation errors
- Environment variables configured

After completion: Update this task status to [x] in tasks.md
```

### Task 1.2: å®ç°åç«¯ WebSocket ç«¯ç‚¹
- [ ] **åˆ›å»º WebSocket è·¯ç”±**
  - æ–‡ä»¶: `backend/api/websocket.py`
  - å®ç° `/ws/transcribe` ç«¯ç‚¹
  - å¤„ç†è¿æ¥ã€æ–­å¼€ã€æ¶ˆæ¯æ¥æ”¶

- [ ] **åˆ›å»ºè½¬å½•æœåŠ¡**
  - æ–‡ä»¶: `backend/services/transcription_service.py`
  - é›†æˆ Gemini Live API
  - å®ç°éŸ³é¢‘æ¥æ”¶å’Œè½¬å½•é€»è¾‘
  - æ·»åŠ é”™è¯¯å¤„ç†å’Œæ—¥å¿—

**_Prompt:**
```
Role: Backend Python developer specializing in real-time communication

Task: Implement WebSocket endpoint for real-time audio transcription
- Create WebSocket route at /ws/transcribe
- Integrate Google Gemini Live API for audio transcription
- Handle audio chunks (base64 encoded PCM)
- Send transcription results back to client
- Implement proper error handling and logging

Restrictions:
- Use asyncio for all I/O operations
- Audio format must be 16-bit PCM, 16kHz, mono
- Handle WebSocket disconnections gracefully
- Log all errors and important events

_Requirements: FR-1.1
_Leverage: design.md section 4.1.2 for code structure

Success:
- WebSocket accepts connections
- Can receive audio chunks from client
- Sends transcription results in correct JSON format
- No memory leaks during long sessions

After completion: Update this task status to [x] in tasks.md
```

### Task 1.3: å®ç°å‰ç«¯éŸ³é¢‘æ•è·
- [ ] **åˆ›å»ºéŸ³é¢‘å½•åˆ¶ Hook**
  - æ–‡ä»¶: `frontend/src/hooks/useAudioRecorder.ts`
  - ä½¿ç”¨ Web Audio API æ•è·éº¦å…‹é£éŸ³é¢‘
  - è½¬æ¢ä¸º 16-bit PCM æ ¼å¼
  - å®ç°å¼€å§‹/æš‚åœ/åœæ­¢åŠŸèƒ½

- [ ] **åˆ›å»º WebSocket Hook**
  - æ–‡ä»¶: `frontend/src/hooks/useWebSocket.ts`
  - å»ºç«‹ WebSocket è¿æ¥
  - å‘é€éŸ³é¢‘æ•°æ®
  - æ¥æ”¶è½¬å½•ç»“æœ

**_Prompt:**
```
Role: Frontend React developer specializing in Web Audio API

Task: Implement audio capture and WebSocket communication
- Create useAudioRecorder hook to capture microphone audio
- Convert audio to 16-bit PCM, 16kHz, mono format
- Create useWebSocket hook for real-time communication
- Send audio chunks every 100ms
- Receive and parse transcription results

Restrictions:
- Use TypeScript for type safety
- Request microphone permissions properly
- Handle audio format conversion correctly (Float32 to Int16)
- Use base64 encoding for audio data
- Clean up resources on unmount

_Requirements: FR-1.1
_Leverage: design.md section 4.1.1 for audio capture code

Success:
- Microphone permission requested and granted
- Audio captured at 16kHz sample rate
- WebSocket connects successfully
- Audio data sent in correct format
- No audio artifacts or dropouts

After completion: Update this task status to [x] in tasks.md
```

### Task 1.4: å®ç°è½¬å½•æ–‡æœ¬æ˜¾ç¤º UI
- [ ] **åˆ›å»ºåŒæ å¸ƒå±€**
  - æ–‡ä»¶: `frontend/src/components/Layout/MainLayout.tsx`
  - å·¦ä¾§ï¼šè½¬å½•é¢æ¿ï¼ˆ60% å®½åº¦ï¼‰
  - å³ä¾§ï¼šAI å·¥å…·æ ‡ç­¾é¡µï¼ˆ40% å®½åº¦ï¼‰
  - åº•éƒ¨ï¼šæ§åˆ¶æ¡ï¼ˆå›ºå®šé«˜åº¦ 80pxï¼‰

- [ ] **åˆ›å»ºè½¬å½•é¢æ¿ç»„ä»¶**
  - æ–‡ä»¶: `frontend/src/components/Transcript/TranscriptPanel.tsx`
  - æ˜¾ç¤ºè½¬å½•æ–‡æœ¬å—åˆ—è¡¨
  - ä½¿ç”¨è™šæ‹ŸåŒ–åˆ—è¡¨ï¼ˆreact-windowï¼‰
  - æ¯ä¸ªæ–‡æœ¬å—æ˜¾ç¤ºæ—¶é—´æˆ³å’Œæ–‡æœ¬

- [ ] **åˆ›å»ºåº•éƒ¨æ§åˆ¶æ¡**
  - æ–‡ä»¶: `frontend/src/components/Layout/BottomControls.tsx`
  - å¼€å§‹/æš‚åœ/åœæ­¢æŒ‰é’®
  - å½•éŸ³æ—¶é•¿æ˜¾ç¤º
  - è¿æ¥çŠ¶æ€æŒ‡ç¤ºå™¨

**_Prompt:**
```
Role: Frontend React developer with expertise in responsive UI design

Task: Create the main UI layout and transcript display components
- Implement MainLayout with 60-40 split (left: transcript, right: AI tools)
- Create TranscriptPanel with virtualized list for performance
- Create BottomControls with recording buttons and status display
- Use TailwindCSS for styling
- Make layout responsive

Restrictions:
- Use TypeScript interfaces from design.md
- Implement TranscriptBlock interface properly
- Use react-window for virtualization (optimize for 1000+ blocks)
- Follow TailwindCSS best practices
- Ensure accessibility (ARIA labels, keyboard navigation)

_Requirements: FR-1.3, FR-1.1
_Leverage: design.md section 4.3 for component structure

Success:
- Layout renders correctly on desktop (1920x1080)
- Transcript blocks display with timestamps
- Recording buttons are functional
- Virtualized list performs well with 1000+ items
- No layout shifts or flickering

After completion: Update this task status to [x] in tasks.md
```

### Task 1.5: å®ç°å®æ—¶éŸ³é¢‘æ³¢å½¢å¯è§†åŒ–
- [ ] **åˆ›å»ºæ³¢å½¢ç»„ä»¶**
  - æ–‡ä»¶: `frontend/src/components/Transcript/Waveform.tsx`
  - ä½¿ç”¨ Canvas API ç»˜åˆ¶å®æ—¶æ³¢å½¢
  - æ˜¾ç¤ºæœ€è¿‘ 5 ç§’çš„éŸ³é¢‘æŒ¯å¹…
  - å¹³æ»‘åŠ¨ç”»æ•ˆæœ

**_Prompt:**
```
Role: Frontend developer specializing in Canvas API and audio visualization

Task: Create real-time audio waveform visualization component
- Use Canvas API to draw waveform
- Display last 5 seconds of audio amplitude
- Update at 60 FPS for smooth animation
- Use AnalyserNode from Web Audio API
- Add color gradient for visual appeal

Restrictions:
- Canvas size: 300px width x 60px height
- Use requestAnimationFrame for rendering
- Optimize for performance (no memory leaks)
- Clear canvas properly on each frame

_Requirements: FR-1.2
_Leverage: design.md section 4.1.1 for AudioContext integration

Success:
- Waveform displays in real-time
- Smooth 60 FPS animation
- CPU usage < 5% for waveform rendering
- Waveform scales properly with audio volume

After completion: Update this task status to [x] in tasks.md
```

## Phase 2: å¤šè¯­è¨€å®æ—¶ç¿»è¯‘

### Task 2.1: å®ç°è¯­è¨€æ£€æµ‹å’Œç¿»è¯‘
- [ ] **æ·»åŠ è¯­è¨€æ£€æµ‹åŠŸèƒ½**
  - æ–‡ä»¶: `backend/services/transcription_service.py`
  - æ£€æµ‹è½¬å½•æ–‡æœ¬çš„è¯­è¨€ï¼ˆä¸­æ–‡ã€æ—¥è¯­ã€éŸ©è¯­ç­‰ï¼‰
  - ä½¿ç”¨å­—ç¬¦èŒƒå›´æˆ–é›†æˆ langdetect åº“

- [ ] **å®ç°è‡ªåŠ¨ç¿»è¯‘**
  - è°ƒç”¨ Gemini API å°†éè‹±æ–‡æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡
  - ä¼˜åŒ–ç¿»è¯‘ Promptï¼ˆä»…è¿”å›ç¿»è¯‘ç»“æœï¼‰
  - ç¼“å­˜ç¿»è¯‘ç»“æœé¿å…é‡å¤è°ƒç”¨

**_Prompt:**
```
Role: Backend developer with expertise in NLP and multilingual applications

Task: Implement automatic language detection and translation to English
- Add language detection function (support: zh, ja, ko, es, fr, de, ru)
- Create translation service using Gemini 2.0 Flash API
- If detected language is not English, translate to English automatically
- Include both original and translated text in WebSocket response
- Add translation caching to reduce API calls

Restrictions:
- Translation must be accurate and contextual
- Use efficient language detection (no heavy libraries)
- Translation prompt should output only translated text (no explanations)
- Handle translation API errors gracefully
- Async/await for all API calls

_Requirements: FR-2.1
_Leverage: design.md section 4.1.2 _translate_to_english method

Success:
- Language detection works for major languages (95%+ accuracy)
- Translation completes in < 3 seconds
- Both original and translated text returned to client
- No duplicate translations for same text
- Translation errors don't crash the service

After completion: Update this task status to [x] in tasks.md
```

### Task 2.2: å®ç°å‰ç«¯è¯­è¨€åˆ‡æ¢ UI
- [ ] **åˆ›å»ºè¯­è¨€åˆ‡æ¢ç»„ä»¶**
  - æ–‡ä»¶: `frontend/src/components/Transcript/LanguageToggle.tsx`
  - ä¸‰ä¸ªé€‰é¡¹ï¼š["åŸæ–‡" | "è‹±æ–‡ç¿»è¯‘" | "åŒè¯­å¯¹ç…§"]
  - ä½¿ç”¨ Tab æˆ– Radio Button æ ·å¼

- [ ] **æ›´æ–°è½¬å½•å—æ˜¾ç¤º**
  - æ–‡ä»¶: `frontend/src/components/Transcript/TranscriptBlock.tsx`
  - æ ¹æ®é€‰æ‹©æ˜¾ç¤ºä¸åŒå†…å®¹
  - åŒè¯­æ¨¡å¼ä¸‹æ˜¾ç¤ºåŸæ–‡å’Œç¿»è¯‘ï¼ˆä¸Šä¸‹å¸ƒå±€ï¼‰
  - æ·»åŠ è¯­è¨€æ ‡ç­¾ï¼ˆå¦‚ "ğŸ‡¨ğŸ‡³ ä¸­æ–‡"ï¼‰

**_Prompt:**
```
Role: Frontend React developer with expertise in internationalization

Task: Create language toggle UI for switching between original and translated text
- Create LanguageToggle component with 3 modes (Original, English, Bilingual)
- Update TranscriptBlock to display based on selected mode
- In bilingual mode, show both texts with clear separation
- Add language badges/flags for visual clarity
- Smooth transitions between modes

Restrictions:
- Use TypeScript enums for ViewMode type
- Use TailwindCSS for styling (no custom CSS)
- Ensure text is readable in all modes
- Handle long text with proper wrapping
- Accessible for screen readers

_Requirements: FR-2.2
_Leverage: TranscriptBlock interface from design.md

Success:
- Toggle switches between 3 modes smoothly
- Original text displays correctly for all languages
- Translated English text is clear and readable
- Bilingual mode shows both texts clearly
- Language badges display for non-English text

After completion: Update this task status to [x] in tasks.md
```

## Phase 3: ç¬”è®°åŠŸèƒ½ä¸ä¿å­˜ä¸‹è½½

### Task 3.1: é›†æˆå¯Œæ–‡æœ¬ç¼–è¾‘å™¨
- [ ] **å®‰è£…å¹¶é…ç½® Tiptap ç¼–è¾‘å™¨**
  - æ–‡ä»¶: `frontend/src/components/Notes/NotesEditor.tsx`
  - é›†æˆ Tiptapï¼ˆæˆ– Quillï¼‰
  - é…ç½®å·¥å…·æ ï¼šåŠ ç²—ã€æ–œä½“ã€æ ‡é¢˜ã€åˆ—è¡¨ã€ä»£ç å—

- [ ] **å®ç°ç¬”è®°è‡ªåŠ¨ä¿å­˜**
  - ä½¿ç”¨ localStorage ä¿å­˜ç¬”è®°å†…å®¹
  - é˜²æŠ–ä¿å­˜ï¼ˆ1 ç§’å»¶è¿Ÿï¼‰

**_Prompt:**
```
Role: Frontend React developer specializing in rich text editors

Task: Integrate Tiptap rich text editor for lecture notes
- Install and configure Tiptap editor
- Create toolbar with essential formatting options (bold, italic, heading, lists, code)
- Implement auto-save to localStorage (debounced by 1 second)
- Load saved notes on component mount
- Add export functionality (Markdown format)

Restrictions:
- Use Tiptap (not Quill or other editors)
- Keep toolbar minimal but functional
- Debounce saves to avoid excessive localStorage writes
- Handle edge cases (empty notes, invalid JSON)
- TypeScript for all code

_Requirements: FR-3.1
_Leverage: design.md section 4.3 for component structure

Success:
- Editor loads with proper toolbar
- Can format text with all toolbar options
- Notes auto-save every 1 second after editing
- Notes persist after page refresh
- Can export notes as Markdown

After completion: Update this task status to [x] in tasks.md
```

### Task 3.2: å®ç°å½•éŸ³å’Œè½¬å½•ä¸‹è½½
- [ ] **å½•éŸ³æ–‡ä»¶ä¿å­˜**
  - æ–‡ä»¶: `frontend/src/hooks/useAudioRecorder.ts`
  - ä½¿ç”¨ MediaRecorder å½•åˆ¶ WebM/WAV æ ¼å¼
  - åœæ­¢å½•éŸ³æ—¶ç”Ÿæˆ Blob

- [ ] **ä¸‹è½½åŠŸèƒ½å®ç°**
  - æ–‡ä»¶: `frontend/src/utils/download.utils.ts`
  - ä¸‹è½½å½•éŸ³æ–‡ä»¶ï¼ˆaudio.webmï¼‰
  - ä¸‹è½½è½¬å½•æ–‡æœ¬ï¼ˆtranscript.json æˆ– transcript.txtï¼‰
  - æ–‡ä»¶ååŒ…å«æ—¥æœŸæ—¶é—´

**_Prompt:**
```
Role: Frontend developer with expertise in browser APIs

Task: Implement audio recording and download functionality
- Record audio using MediaRecorder API (WebM format)
- Save recorded audio as Blob
- Create download function for audio file
- Create download function for transcript (JSON with timestamps and translations)
- Generate filenames with timestamp (e.g., "lecture_2025-10-14_14-30.webm")

Restrictions:
- Use MediaRecorder with proper MIME type support checking
- Handle browsers that don't support WebM (fallback to WAV)
- Transcript JSON should include all fields (original, translated, timestamps)
- Download should work without server-side processing
- Clean up Blob URLs after download

_Requirements: FR-3.2
_Leverage: design.md section 2.1 for TranscriptBlock structure

Success:
- Audio records in WebM or WAV format
- Download button appears after recording stops
- Audio file downloads with correct extension
- Transcript JSON includes all data fields
- Filenames include date and time
- No memory leaks from Blob URLs

After completion: Update this task status to [x] in tasks.md
```

## Phase 4: é«˜çº§ AI å­¦ä¹ å·¥å…·

### Task 4.1: å®ç°é—ªå¡ç”Ÿæˆ API
- [ ] **åˆ›å»ºé—ªå¡ç”Ÿæˆç«¯ç‚¹**
  - æ–‡ä»¶: `backend/api/generate.py`
  - å®ç° `POST /api/generate/flashcards`
  - ä½¿ç”¨ Gemini 2.0 Flash ç”Ÿæˆé—ªå¡
  - ç²¾å¿ƒè®¾è®¡ Promptï¼ˆå‚è€ƒ design.mdï¼‰

**_Prompt:**
```
Role: Backend developer with expertise in AI prompt engineering

Task: Create API endpoint for AI-generated flashcards
- Implement POST /api/generate/flashcards endpoint
- Accept transcript text and count parameter
- Use Gemini 2.0 Flash API to generate flashcards
- Parse AI response into structured JSON
- Handle API errors and edge cases

Restrictions:
- Use prompt from design.md section 4.2.1
- Return JSON array of flashcards (front/back structure)
- Validate AI response format before returning
- Set API timeout to 30 seconds
- Log all generation requests

_Requirements: FR-4.1
_Leverage: design.md FLASHCARD_PROMPT

Success:
- Endpoint accepts POST requests
- Generates 10-20 flashcards based on input
- Returns valid JSON with front/back fields
- Handles empty or short transcripts gracefully
- Response time < 10 seconds for typical input

After completion: Update this task status to [x] in tasks.md
```

### Task 4.2: å®ç°æµ‹éªŒç”Ÿæˆ API
- [ ] **åˆ›å»ºæµ‹éªŒç”Ÿæˆç«¯ç‚¹**
  - æ–‡ä»¶: `backend/api/generate.py`
  - å®ç° `POST /api/generate/quiz`
  - ç”Ÿæˆå¤šé€‰é¢˜ï¼ˆ4 ä¸ªé€‰é¡¹ + æ­£ç¡®ç­”æ¡ˆ + è§£æï¼‰

**_Prompt:**
```
Role: Backend developer specializing in educational AI applications

Task: Create API endpoint for AI-generated quiz questions
- Implement POST /api/generate/quiz endpoint
- Accept transcript text and question count
- Use Gemini 2.0 Flash to generate multiple-choice questions
- Each question should have 4 options, correct answer index, and explanation
- Parse and validate AI response

Restrictions:
- Use prompt from design.md section 4.2.2
- Return JSON array with question structure
- Ensure questions are challenging and diverse
- Validate that correctAnswer index is 0-3
- Handle JSON parsing errors from AI

_Requirements: FR-4.2
_Leverage: design.md QUIZ_PROMPT

Success:
- Endpoint generates 5-10 quiz questions
- Each question has exactly 4 options
- Correct answer index is valid (0-3)
- Explanations are clear and helpful
- Questions cover diverse topics from transcript

After completion: Update this task status to [x] in tasks.md
```

### Task 4.3: å®ç°æ€ç»´å¯¼å›¾ç”Ÿæˆ API
- [ ] **åˆ›å»ºæ€ç»´å¯¼å›¾ç”Ÿæˆç«¯ç‚¹**
  - æ–‡ä»¶: `backend/api/generate.py`
  - å®ç° `POST /api/generate/mindmap`
  - ç”Ÿæˆå±‚çº§ç»“æ„ï¼ˆæ ¹èŠ‚ç‚¹ + å­èŠ‚ç‚¹ï¼‰

**_Prompt:**
```
Role: Backend developer with knowledge of hierarchical data structures

Task: Create API endpoint for AI-generated mind map
- Implement POST /api/generate/mindmap endpoint
- Use Gemini 2.0 Flash to analyze transcript and extract hierarchy
- Return nested JSON structure (root + children)
- Ensure max 3 levels deep for readability

Restrictions:
- Use prompt from design.md section 4.2.3
- Validate hierarchical structure before returning
- Limit to 5-7 main branches
- Each branch should have 2-4 sub-items
- Handle malformed AI responses

_Requirements: FR-4.3
_Leverage: design.md MINDMAP_PROMPT and MindMapNode interface

Success:
- Endpoint generates valid hierarchical structure
- Mind map has clear main topic (root)
- 3-5 major subtopics identified
- Each subtopic has 2-4 key points
- JSON structure matches MindMapNode interface

After completion: Update this task status to [x] in tasks.md
```

### Task 4.4: å®ç°å‰ç«¯ AI å·¥å…· UI
- [ ] **åˆ›å»ºé—ªå¡è§†å›¾**
  - æ–‡ä»¶: `frontend/src/components/AITools/FlashcardsView.tsx`
  - æ˜¾ç¤ºé—ªå¡ï¼ˆç¿»è½¬æ•ˆæœï¼‰
  - ç”ŸæˆæŒ‰é’® + åŠ è½½çŠ¶æ€

- [ ] **åˆ›å»ºæµ‹éªŒè§†å›¾**
  - æ–‡ä»¶: `frontend/src/components/AITools/QuizView.tsx`
  - æ˜¾ç¤ºé¢˜ç›®å’Œé€‰é¡¹
  - æäº¤ç­”æ¡ˆå¹¶æ˜¾ç¤ºå¾—åˆ†

- [ ] **åˆ›å»ºæ€ç»´å¯¼å›¾è§†å›¾**
  - æ–‡ä»¶: `frontend/src/components/AITools/MindMapView.tsx`
  - ä½¿ç”¨ React Flow æ¸²æŸ“æ€ç»´å¯¼å›¾
  - å¯ç¼©æ”¾å’Œæ‹–æ‹½

**_Prompt:**
```
Role: Frontend React developer with UI/UX expertise

Task: Create UI components for AI-generated learning tools
- FlashcardsView: Display flashcards with flip animation
- QuizView: Interactive quiz interface with scoring
- MindMapView: Render hierarchical mind map using React Flow
- Add "Generate" buttons for each tool
- Show loading states during generation

Restrictions:
- Use TypeScript interfaces from design.md
- Use TailwindCSS for styling
- Add proper error handling (API failures)
- Disable generate button during loading
- For MindMapView, use react-flow-renderer library

_Requirements: FR-4.1, FR-4.2, FR-4.3
_Leverage: design.md section 4.3 for component structure

Success:
- Flashcards display and flip smoothly
- Quiz allows selecting options and shows results
- Mind map renders with proper layout
- Generate buttons trigger API calls
- Loading spinners display during generation
- Error messages show if generation fails

After completion: Update this task status to [x] in tasks.md
```

## Phase 5: èŠå¤©æœºå™¨äºº

### Task 5.1: å®ç°èŠå¤© API
- [ ] **åˆ›å»ºèŠå¤©ç«¯ç‚¹**
  - æ–‡ä»¶: `backend/api/chat.py`
  - å®ç° `POST /api/chat`
  - æ¥æ”¶ç”¨æˆ·é—®é¢˜ + è½¬å½•ä¸Šä¸‹æ–‡ + å¯¹è¯å†å²
  - ä½¿ç”¨ Gemini ç”Ÿæˆå›ç­”

**_Prompt:**
```
Role: Backend developer with expertise in conversational AI

Task: Create API endpoint for chat-based Q&A about lecture content
- Implement POST /api/chat endpoint
- Accept user message, transcript context, and chat history
- Use Gemini 2.0 Flash for conversational responses
- Maintain context across multiple turns
- Provide accurate answers based on transcript

Restrictions:
- Include transcript in system context
- Limit history to last 10 messages to avoid token limits
- Format messages properly (user/assistant roles)
- Handle "I don't know" cases gracefully
- Response time < 5 seconds

_Requirements: FR-5.1
_Leverage: design.md API design section 3.2

Success:
- Endpoint accepts POST requests with proper structure
- AI provides relevant answers based on transcript
- Conversation context maintained across turns
- Handles off-topic questions appropriately
- Response is conversational and helpful

After completion: Update this task status to [x] in tasks.md
```

### Task 5.2: å®ç°èŠå¤© UI
- [ ] **åˆ›å»ºèŠå¤©è§†å›¾**
  - æ–‡ä»¶: `frontend/src/components/AITools/ChatView.tsx`
  - èŠå¤©æ¶ˆæ¯åˆ—è¡¨
  - è¾“å…¥æ¡† + å‘é€æŒ‰é’®
  - æ˜¾ç¤ºæ‰“å­—æŒ‡ç¤ºå™¨

**_Prompt:**
```
Role: Frontend React developer specializing in chat interfaces

Task: Create interactive chat UI for Q&A about lecture content
- Display chat history (user and assistant messages)
- Input field with send button
- Auto-scroll to latest message
- Show typing indicator when AI is responding
- Timestamp for each message

Restrictions:
- Use TailwindCSS for styling
- Different styles for user vs assistant messages
- Handle long messages with proper text wrapping
- Disable input while waiting for response
- Clear, accessible UI (ARIA labels)

_Requirements: FR-5.1
_Leverage: design.md section 4.3 for component structure

Success:
- Messages display in chronological order
- User can type and send questions
- AI responses appear with typing animation
- Auto-scrolls to bottom on new message
- Handles long conversations (100+ messages)

After completion: Update this task status to [x] in tasks.md
```

## Phase 6: å¥å£®æ€§ä¸æµ‹è¯•

### Task 6.1: å®ç° WebSocket å¿ƒè·³å’Œé‡è¿
- [ ] **åç«¯å¿ƒè·³æœºåˆ¶**
  - æ–‡ä»¶: `backend/api/websocket.py`
  - æ¯ 30 ç§’å‘é€ ping æ¶ˆæ¯
  - æ£€æµ‹å®¢æˆ·ç«¯è¿æ¥çŠ¶æ€

- [ ] **å‰ç«¯é‡è¿é€»è¾‘**
  - æ–‡ä»¶: `frontend/src/hooks/useWebSocket.ts`
  - æ–­çº¿è‡ªåŠ¨é‡è¿ï¼ˆæœ€å¤š 5 æ¬¡ï¼‰
  - æŒ‡æ•°é€€é¿ç­–ç•¥ï¼ˆ2s, 4s, 8s...ï¼‰
  - éŸ³é¢‘ç¼“å†²é˜²æ­¢æ•°æ®ä¸¢å¤±

**_Prompt:**
```
Role: Full-stack developer specializing in real-time communication reliability

Task: Implement WebSocket heartbeat and reconnection logic
- Backend: Send ping every 30 seconds, disconnect inactive clients
- Frontend: Auto-reconnect on disconnection (max 5 attempts)
- Use exponential backoff for reconnection (2s, 4s, 8s, 16s, 32s)
- Buffer audio chunks during disconnection (max 10 seconds)
- Send buffered audio after reconnection

Restrictions:
- Don't spam reconnection attempts
- Clear timers/intervals on cleanup
- Show connection status to user ("Connecting...", "Connected", "Disconnected")
- Log all connection events
- Handle edge case: user stops recording during reconnection

_Requirements: FR-6.1
_Leverage: design.md section 6.1 for error handling code

Success:
- WebSocket stays alive with heartbeat
- Auto-reconnects after network interruption
- Buffered audio sent successfully after reconnection
- Connection status indicator updates correctly
- No infinite reconnection loops

After completion: Update this task status to [x] in tasks.md
```

### Task 6.2: é•¿æ—¶ç¨‹å½•éŸ³æµ‹è¯•
- [ ] **æ€§èƒ½æµ‹è¯•**
  - å½•åˆ¶ 45 åˆ†é’ŸéŸ³é¢‘
  - ç›‘æ§å†…å­˜ä½¿ç”¨ï¼ˆå‰ç«¯ + åç«¯ï¼‰
  - æ£€æŸ¥è½¬å½•å‡†ç¡®æ€§
  - æµ‹è¯•ç½‘ç»œæ–­çº¿æ¢å¤

**_Prompt:**
```
Role: QA engineer and performance tester

Task: Conduct comprehensive 45-minute recording test
- Start recording and let it run for 45 minutes
- Monitor frontend memory usage (Chrome DevTools)
- Monitor backend memory and CPU (top/htop)
- Simulate network interruptions at 15min and 30min
- Verify all transcript blocks are captured
- Check for memory leaks or performance degradation

Restrictions:
- Use realistic audio input (lecture recording or live speech)
- Run test in production-like environment
- Document any issues found
- Measure transcript accuracy (spot-check 10 random blocks)
- Test in Chrome, Firefox, and Safari

_Requirements: FR-6.2
_Leverage: None (testing task)

Success:
- System runs for full 45 minutes without crashes
- Frontend memory usage < 500MB
- Backend memory usage < 300MB
- CPU usage < 30% average
- Transcript accuracy > 90%
- All network interruptions recovered successfully
- No audio gaps or missing transcripts

After completion: Update this task status to [x] in tasks.md
```

### Task 6.3: é”™è¯¯å¤„ç†å’Œç”¨æˆ·æç¤º
- [ ] **å®Œå–„é”™è¯¯å¤„ç†**
  - æ‰€æœ‰ API è°ƒç”¨æ·»åŠ  try-catch
  - å‰ç«¯æ˜¾ç¤ºå‹å¥½é”™è¯¯æ¶ˆæ¯ï¼ˆToast é€šçŸ¥ï¼‰
  - åç«¯è®°å½•è¯¦ç»†é”™è¯¯æ—¥å¿—

**_Prompt:**
```
Role: Full-stack developer focused on production-ready code

Task: Add comprehensive error handling and user notifications
- Wrap all API calls in try-catch blocks
- Display user-friendly error messages (use toast notifications)
- Backend: Log all errors with context (stack trace, request info)
- Handle common errors: API key invalid, rate limits, network failures
- Add fallback UI for failed operations

Restrictions:
- Don't expose sensitive info in error messages
- Log errors server-side but show friendly messages client-side
- Use a toast library (e.g., react-hot-toast)
- Provide actionable error messages ("Try again", "Check connection")
- Don't crash the app on errors

_Requirements: All FRs (cross-cutting concern)
_Leverage: design.md section 6 for error handling patterns

Success:
- No unhandled promise rejections
- User sees clear error messages
- Errors logged to backend with full context
- App remains functional after errors
- Rate limit errors handled gracefully

After completion: Update this task status to [x] in tasks.md
```

## Phase 7: æ–‡æ¡£å’Œéƒ¨ç½²å‡†å¤‡

### Task 7.1: ç¼–å†™ README å’Œæ–‡æ¡£
- [ ] **åˆ›å»ºé¡¹ç›® README**
  - æ–‡ä»¶: `README.md`
  - é¡¹ç›®ä»‹ç»ã€åŠŸèƒ½åˆ—è¡¨
  - å®‰è£…å’Œè¿è¡Œè¯´æ˜
  - ç¯å¢ƒå˜é‡é…ç½®
  - æ¶æ„å›¾å’ŒæŠ€æœ¯æ ˆ

- [ ] **API æ–‡æ¡£**
  - ä½¿ç”¨ FastAPI è‡ªåŠ¨ç”Ÿæˆçš„ Swagger UI
  - æ·»åŠ è¯¦ç»†çš„ docstrings

**_Prompt:**
```
Role: Technical writer and documentation specialist

Task: Create comprehensive project documentation
- Write README.md with project overview, features, setup instructions
- Include environment variables setup (.env.example)
- Add architecture diagram (ASCII or link to design.md)
- Document all API endpoints (backend README or docstrings)
- Add troubleshooting section

Restrictions:
- Use clear, concise language
- Include code examples for setup
- Markdown formatting for readability
- Link to design.md and requirements.md for details
- Keep it beginner-friendly

_Requirements: None (documentation task)
_Leverage: design.md and requirements.md for content

Success:
- README has all sections (intro, features, setup, usage, troubleshooting)
- Anyone can set up the project by following README
- Environment variables clearly explained
- API documentation accessible via Swagger UI
- Links to spec documents included

After completion: Update this task status to [x] in tasks.md
```

### Task 7.2: å‡†å¤‡ Git ä»“åº“å’Œéƒ¨ç½²
- [ ] **åˆå§‹åŒ– Git ä»“åº“**
  - æ–‡ä»¶: `.gitignore`
  - æäº¤æ‰€æœ‰ä»£ç åˆ° Git
  - åˆ›å»ºæ¸…æ™°çš„ commit messages

- [ ] **æ¨é€åˆ° GitHub**
  - æ¨é€åˆ° https://github.com/kimiguo1109/class_recorder.git
  - æ·»åŠ æœ‰æ„ä¹‰çš„ commit å†å²

**_Prompt:**
```
Role: DevOps engineer preparing repository for deployment

Task: Initialize Git repository and push to GitHub
- Create .gitignore for Python and Node.js
- Initialize git in project root
- Add all files with clear commit messages
- Push to remote: https://github.com/kimiguo1109/class_recorder.git
- Create branches if needed (main for production-ready code)

Restrictions:
- Do NOT commit .env files or API keys
- Do NOT commit node_modules/ or __pycache__/
- Write descriptive commit messages
- Push all phases incrementally (not one big commit)
- Tag major milestones (v1.0-mvp, v2.0-translation, etc.)

_Requirements: None (deployment task)
_Leverage: None

Success:
- Git repository initialized
- .gitignore configured properly
- All code committed with clear messages
- Code pushed to GitHub successfully
- No sensitive data in repository

After completion: Update this task status to [x] in tasks.md
```

---

## Summary

**Total Tasks: 18**

- Phase 1 (MVP): 5 tasks
- Phase 2 (Translation): 2 tasks
- Phase 3 (Notes): 2 tasks
- Phase 4 (AI Tools): 4 tasks
- Phase 5 (Chat): 2 tasks
- Phase 6 (Testing): 3 tasks
- Phase 7 (Documentation): 2 tasks

**Estimated Timeline: 8-10 days**

**Completion Tracking:**
- [ ] Phase 1: Core Transcription MVP
- [ ] Phase 2: Multilingual Translation
- [ ] Phase 3: Notes and Download
- [ ] Phase 4: AI Learning Tools
- [ ] Phase 5: Chatbot
- [ ] Phase 6: Testing and Stability
- [ ] Phase 7: Documentation and Deployment

